{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gopi/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through env. To show the simulation running, you can use env.render() to render one frame. Passing in an action as an integer to env.step will generate the next step in the simulation. You can see how many actions are possible from env.action_space and to get a random action you can use env.action_space.sample(). This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "rewards = []\n",
    "# env.step(env.action_space.sample())\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    state, reward, done , info = env.step(env.action_space.sample())\n",
    "    rewards.append(reward)\n",
    "    if done:\n",
    "#         print(rewards[:10])\n",
    "        rewards = []\n",
    "        env.reset()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07974487,  0.17801903, -0.07881684, -0.37991832])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "We train our Q-learning agent using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "where $s$ is a state, $a$ is an action, and $s'$ is the next state from state $s$ and action $a$.\n",
    "\n",
    "Before we used this equation to learn values for a Q-table. However, for this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Instead of using a table then, we'll replace it with a neural network that will approximate the Q-table lookup function.\n",
    "\n",
    "<img src=\"assets/deep-q-learning.png\" width=450px>\n",
    "\n",
    "Now, our Q value, $Q(s, a)$ is calculated by passing in a state to the network. The output will be Q-values for each available action, with fully connected hidden layers.\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "As I showed before, we can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$.\n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ network to run the optimizer and update the weights.\n",
    "\n",
    "Below is my implementation of the Q-network. I used two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "    def __init__(self , learning_rate=0.01,\n",
    "                state_size=4, \n",
    "                action_size=2,\n",
    "                hidden_size=10,\n",
    "                name='QNetwork'):\n",
    "        # state inputs to the Q-Network\n",
    "        # https://stackoverflow.com/questions/35919020/whats-the-difference-of-name-scope-and-a-variable-scope-in-tensorflow\n",
    "        with tf.variable_scope(name):\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, state_size], name='inputs')\n",
    "        \n",
    "            # One hot encode the actions to latte choose the Q-Value for the action\n",
    "            self.actions_ = tf.placeholder(tf.int32 , [None] , name=\"actions\") # action value is 0 or 1 , one_hot take int values\n",
    "            one_hot_actions = tf.one_hot(self.actions_, action_size)\n",
    "            \n",
    "            # Target Q values for training\n",
    "            self.targetQs_ = tf.placeholder(tf.float32, [None] , name=\"targetQ\")\n",
    "            \n",
    "            # ReLU hidden layer\n",
    "            # See tf.layers.dense() tf.contrib.layers.fully_connected()\n",
    "            self.fc1 = tf.contrib.layers.fully_connected(self.inputs_ , hidden_size) # activation relu applied by default \n",
    "            self.fc2 = tf.contrib.layers.fully_connected(self.fc1 , hidden_size)\n",
    "            \n",
    "            # Linear output layer\n",
    "            self.output = tf.contrib.layers.fully_connected(self.fc2 , action_size , activation_fn=None)\n",
    "            \n",
    "            \n",
    "            ## Train with loss (targetQ - Q)^2\n",
    "            # output has length 2, for two actions. This next line chooses\n",
    "            # one value from output (per row) according to the one-hot encoded actions.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output , one_hot_actions), axis=1)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on.\n",
    "\n",
    "Here, we'll create a Memory object that will store our experiences, our transitions $ < s, a, r, s' > $. This memory will have a maxmium capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $< s, a, r, s' >$ and train on those.\n",
    "\n",
    "Below, I've implemented a Memory object. If you're unfamiliar with deque, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self , max_size= 1000):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        index = np.random.choice(np.arange(len(self.buffer)),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        return [self.buffer[idx] for idx in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Exploitation\n",
    "To learn about the environment and rules of the game, the agent needs to explore by taking random actions. We'll do this by choosing a random action with some probability $\\epsilon$ (epsilon). That is, with some probability $\\epsilon$ the agent will make a random action and with probability $1 - \\epsilon$, the agent will choose an action from $Q(s,a)$. This is called an $\\epsilon$-greedy policy.\n",
    "\n",
    "At first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called exploitation. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training.\n",
    "\n",
    "## Q-Learning training algorithm\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network. We'll train the network in episodes. One episode is one simulation of the game. For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "Initialize the memory $D$\n",
    "\n",
    "Initialize the action-value network $Q$ with random weights\n",
    "\n",
    "For episode = 1, $M$ do\n",
    "\n",
    "For $t$, $T$ do\n",
    "\n",
    "With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s,a)$\n",
    "\n",
    "Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "\n",
    "Store transition $ < s_t, a_t, r_{t+1}, s_{t+1} > $ in memory $D$\n",
    "\n",
    "Sample random mini-batch from $D$: \n",
    "$< s_j, a_j, r_j, s'_j >$\n",
    "\n",
    "Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "\n",
    "Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "\n",
    "endfor\n",
    "\n",
    "endfor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 700          # max number of episodes to learn from\n",
    "max_steps = 200               # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 20                # experience mini-batch size\n",
    "pretrain_length = batch_size   # number experiences to pretrain the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = QNetwork(name=\"main\" , hidden_size=hidden_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate the experience memory\n",
    "Here I'm re-initializing the simulation and pre-populating the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "# Take one random step to get the pole and cart moving\n",
    "state , reward, done, info = env.step(env.action_space.sample())\n",
    "\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for i in range(pretrain_length):\n",
    "    #env.render()\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # start new episode\n",
    "        env.reset()\n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    \n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state,action, reward , next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Below we'll train our agent. If you want to watch it train, uncomment the env.render() line. This is slow because it's rendering the frames slower than the network can train. But, it's cool to watch the agent get better at the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Total reward: 9.0 Training loss: 1.0676 Explore Prob: 0.9991\n",
      "Episode: 2 Total reward: 34.0 Training loss: 1.0862 Explore Prob: 0.9958\n",
      "Episode: 3 Total reward: 20.0 Training loss: 1.0174 Explore Prob: 0.9938\n",
      "Episode: 4 Total reward: 11.0 Training loss: 1.0658 Explore Prob: 0.9927\n",
      "Episode: 5 Total reward: 13.0 Training loss: 1.0433 Explore Prob: 0.9914\n",
      "Episode: 6 Total reward: 11.0 Training loss: 0.9909 Explore Prob: 0.9903\n",
      "Episode: 7 Total reward: 18.0 Training loss: 1.0400 Explore Prob: 0.9886\n",
      "Episode: 8 Total reward: 14.0 Training loss: 1.1450 Explore Prob: 0.9872\n",
      "Episode: 9 Total reward: 25.0 Training loss: 1.0098 Explore Prob: 0.9848\n",
      "Episode: 10 Total reward: 23.0 Training loss: 1.0010 Explore Prob: 0.9825\n",
      "Episode: 11 Total reward: 12.0 Training loss: 1.0564 Explore Prob: 0.9814\n",
      "Episode: 12 Total reward: 16.0 Training loss: 1.1569 Explore Prob: 0.9798\n",
      "Episode: 13 Total reward: 26.0 Training loss: 1.2595 Explore Prob: 0.9773\n",
      "Episode: 14 Total reward: 30.0 Training loss: 1.2509 Explore Prob: 0.9744\n",
      "Episode: 15 Total reward: 9.0 Training loss: 1.1234 Explore Prob: 0.9735\n",
      "Episode: 16 Total reward: 15.0 Training loss: 1.1662 Explore Prob: 0.9721\n",
      "Episode: 17 Total reward: 17.0 Training loss: 1.0963 Explore Prob: 0.9705\n",
      "Episode: 18 Total reward: 22.0 Training loss: 1.2589 Explore Prob: 0.9683\n",
      "Episode: 19 Total reward: 9.0 Training loss: 1.4278 Explore Prob: 0.9675\n",
      "Episode: 20 Total reward: 13.0 Training loss: 1.3750 Explore Prob: 0.9662\n",
      "Episode: 21 Total reward: 39.0 Training loss: 1.6295 Explore Prob: 0.9625\n",
      "Episode: 22 Total reward: 38.0 Training loss: 1.6922 Explore Prob: 0.9589\n",
      "Episode: 23 Total reward: 12.0 Training loss: 1.8737 Explore Prob: 0.9578\n",
      "Episode: 24 Total reward: 14.0 Training loss: 1.5483 Explore Prob: 0.9564\n",
      "Episode: 25 Total reward: 14.0 Training loss: 1.8093 Explore Prob: 0.9551\n",
      "Episode: 26 Total reward: 16.0 Training loss: 1.4929 Explore Prob: 0.9536\n",
      "Episode: 27 Total reward: 18.0 Training loss: 2.8377 Explore Prob: 0.9519\n",
      "Episode: 28 Total reward: 15.0 Training loss: 1.7606 Explore Prob: 0.9505\n",
      "Episode: 29 Total reward: 9.0 Training loss: 2.1780 Explore Prob: 0.9496\n",
      "Episode: 30 Total reward: 14.0 Training loss: 2.7413 Explore Prob: 0.9483\n",
      "Episode: 31 Total reward: 17.0 Training loss: 3.6335 Explore Prob: 0.9467\n",
      "Episode: 32 Total reward: 41.0 Training loss: 7.3617 Explore Prob: 0.9429\n",
      "Episode: 33 Total reward: 55.0 Training loss: 2.0586 Explore Prob: 0.9378\n",
      "Episode: 34 Total reward: 19.0 Training loss: 3.1783 Explore Prob: 0.9360\n",
      "Episode: 35 Total reward: 38.0 Training loss: 9.5792 Explore Prob: 0.9325\n",
      "Episode: 36 Total reward: 14.0 Training loss: 7.1621 Explore Prob: 0.9312\n",
      "Episode: 37 Total reward: 10.0 Training loss: 3.7346 Explore Prob: 0.9303\n",
      "Episode: 38 Total reward: 14.0 Training loss: 4.7434 Explore Prob: 0.9290\n",
      "Episode: 39 Total reward: 19.0 Training loss: 4.0611 Explore Prob: 0.9273\n",
      "Episode: 40 Total reward: 30.0 Training loss: 5.0639 Explore Prob: 0.9245\n",
      "Episode: 41 Total reward: 12.0 Training loss: 8.4536 Explore Prob: 0.9234\n",
      "Episode: 42 Total reward: 30.0 Training loss: 4.7890 Explore Prob: 0.9207\n",
      "Episode: 43 Total reward: 12.0 Training loss: 4.6764 Explore Prob: 0.9196\n",
      "Episode: 44 Total reward: 13.0 Training loss: 10.7062 Explore Prob: 0.9184\n",
      "Episode: 45 Total reward: 7.0 Training loss: 3.6476 Explore Prob: 0.9178\n",
      "Episode: 46 Total reward: 16.0 Training loss: 17.5042 Explore Prob: 0.9163\n",
      "Episode: 47 Total reward: 15.0 Training loss: 16.9526 Explore Prob: 0.9150\n",
      "Episode: 48 Total reward: 24.0 Training loss: 10.3975 Explore Prob: 0.9128\n",
      "Episode: 49 Total reward: 28.0 Training loss: 32.0375 Explore Prob: 0.9103\n",
      "Episode: 50 Total reward: 12.0 Training loss: 7.8665 Explore Prob: 0.9092\n",
      "Episode: 51 Total reward: 9.0 Training loss: 7.6995 Explore Prob: 0.9084\n",
      "Episode: 52 Total reward: 9.0 Training loss: 19.3961 Explore Prob: 0.9076\n",
      "Episode: 53 Total reward: 60.0 Training loss: 34.5029 Explore Prob: 0.9022\n",
      "Episode: 54 Total reward: 20.0 Training loss: 27.4769 Explore Prob: 0.9004\n",
      "Episode: 55 Total reward: 14.0 Training loss: 10.3780 Explore Prob: 0.8992\n",
      "Episode: 56 Total reward: 21.0 Training loss: 55.1224 Explore Prob: 0.8973\n",
      "Episode: 57 Total reward: 12.0 Training loss: 14.3276 Explore Prob: 0.8963\n",
      "Episode: 58 Total reward: 22.0 Training loss: 26.3957 Explore Prob: 0.8943\n",
      "Episode: 59 Total reward: 19.0 Training loss: 13.0344 Explore Prob: 0.8926\n",
      "Episode: 60 Total reward: 14.0 Training loss: 16.4620 Explore Prob: 0.8914\n",
      "Episode: 61 Total reward: 19.0 Training loss: 48.8923 Explore Prob: 0.8897\n",
      "Episode: 62 Total reward: 70.0 Training loss: 15.4167 Explore Prob: 0.8836\n",
      "Episode: 63 Total reward: 50.0 Training loss: 17.9527 Explore Prob: 0.8792\n",
      "Episode: 64 Total reward: 14.0 Training loss: 116.1188 Explore Prob: 0.8780\n",
      "Episode: 65 Total reward: 26.0 Training loss: 22.9847 Explore Prob: 0.8758\n",
      "Episode: 66 Total reward: 19.0 Training loss: 47.5159 Explore Prob: 0.8741\n",
      "Episode: 67 Total reward: 10.0 Training loss: 16.2139 Explore Prob: 0.8733\n",
      "Episode: 68 Total reward: 16.0 Training loss: 18.0148 Explore Prob: 0.8719\n",
      "Episode: 69 Total reward: 14.0 Training loss: 33.4789 Explore Prob: 0.8707\n",
      "Episode: 70 Total reward: 47.0 Training loss: 290.6498 Explore Prob: 0.8666\n",
      "Episode: 71 Total reward: 13.0 Training loss: 116.1601 Explore Prob: 0.8655\n",
      "Episode: 72 Total reward: 9.0 Training loss: 58.2717 Explore Prob: 0.8647\n",
      "Episode: 73 Total reward: 30.0 Training loss: 198.3058 Explore Prob: 0.8622\n",
      "Episode: 74 Total reward: 20.0 Training loss: 34.5881 Explore Prob: 0.8605\n",
      "Episode: 75 Total reward: 21.0 Training loss: 255.2105 Explore Prob: 0.8587\n",
      "Episode: 76 Total reward: 16.0 Training loss: 35.6497 Explore Prob: 0.8573\n",
      "Episode: 77 Total reward: 15.0 Training loss: 225.1439 Explore Prob: 0.8561\n",
      "Episode: 78 Total reward: 16.0 Training loss: 26.3921 Explore Prob: 0.8547\n",
      "Episode: 79 Total reward: 28.0 Training loss: 77.4494 Explore Prob: 0.8524\n",
      "Episode: 80 Total reward: 12.0 Training loss: 177.1085 Explore Prob: 0.8513\n",
      "Episode: 81 Total reward: 12.0 Training loss: 95.6050 Explore Prob: 0.8503\n",
      "Episode: 82 Total reward: 11.0 Training loss: 60.9837 Explore Prob: 0.8494\n",
      "Episode: 83 Total reward: 15.0 Training loss: 152.2972 Explore Prob: 0.8482\n",
      "Episode: 84 Total reward: 9.0 Training loss: 150.0727 Explore Prob: 0.8474\n",
      "Episode: 85 Total reward: 32.0 Training loss: 40.8300 Explore Prob: 0.8447\n",
      "Episode: 86 Total reward: 34.0 Training loss: 94.1447 Explore Prob: 0.8419\n",
      "Episode: 87 Total reward: 13.0 Training loss: 286.5644 Explore Prob: 0.8408\n",
      "Episode: 88 Total reward: 28.0 Training loss: 258.6818 Explore Prob: 0.8385\n",
      "Episode: 89 Total reward: 10.0 Training loss: 101.6369 Explore Prob: 0.8377\n",
      "Episode: 90 Total reward: 9.0 Training loss: 45.2433 Explore Prob: 0.8369\n",
      "Episode: 91 Total reward: 14.0 Training loss: 66.8272 Explore Prob: 0.8358\n",
      "Episode: 92 Total reward: 24.0 Training loss: 42.5037 Explore Prob: 0.8338\n",
      "Episode: 93 Total reward: 9.0 Training loss: 64.1167 Explore Prob: 0.8330\n",
      "Episode: 94 Total reward: 13.0 Training loss: 358.8766 Explore Prob: 0.8320\n",
      "Episode: 95 Total reward: 9.0 Training loss: 103.4201 Explore Prob: 0.8312\n",
      "Episode: 96 Total reward: 12.0 Training loss: 137.2899 Explore Prob: 0.8302\n",
      "Episode: 97 Total reward: 21.0 Training loss: 90.3527 Explore Prob: 0.8285\n",
      "Episode: 98 Total reward: 15.0 Training loss: 143.0339 Explore Prob: 0.8273\n",
      "Episode: 99 Total reward: 18.0 Training loss: 329.0408 Explore Prob: 0.8258\n",
      "Episode: 100 Total reward: 15.0 Training loss: 114.7019 Explore Prob: 0.8246\n",
      "Episode: 101 Total reward: 29.0 Training loss: 316.0923 Explore Prob: 0.8222\n",
      "Episode: 102 Total reward: 27.0 Training loss: 73.2639 Explore Prob: 0.8201\n",
      "Episode: 103 Total reward: 12.0 Training loss: 449.2928 Explore Prob: 0.8191\n",
      "Episode: 104 Total reward: 22.0 Training loss: 724.5892 Explore Prob: 0.8173\n",
      "Episode: 105 Total reward: 9.0 Training loss: 156.2285 Explore Prob: 0.8166\n",
      "Episode: 106 Total reward: 20.0 Training loss: 584.2509 Explore Prob: 0.8150\n",
      "Episode: 107 Total reward: 15.0 Training loss: 94.3665 Explore Prob: 0.8138\n",
      "Episode: 108 Total reward: 23.0 Training loss: 72.1050 Explore Prob: 0.8119\n",
      "Episode: 109 Total reward: 15.0 Training loss: 507.7068 Explore Prob: 0.8107\n",
      "Episode: 110 Total reward: 12.0 Training loss: 89.2289 Explore Prob: 0.8098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 111 Total reward: 24.0 Training loss: 145.3499 Explore Prob: 0.8078\n",
      "Episode: 112 Total reward: 9.0 Training loss: 1025.8918 Explore Prob: 0.8071\n",
      "Episode: 113 Total reward: 35.0 Training loss: 198.8607 Explore Prob: 0.8043\n",
      "Episode: 114 Total reward: 17.0 Training loss: 100.9967 Explore Prob: 0.8030\n",
      "Episode: 115 Total reward: 8.0 Training loss: 369.5026 Explore Prob: 0.8024\n",
      "Episode: 116 Total reward: 24.0 Training loss: 540.6777 Explore Prob: 0.8005\n",
      "Episode: 117 Total reward: 13.0 Training loss: 1565.1658 Explore Prob: 0.7994\n",
      "Episode: 118 Total reward: 18.0 Training loss: 232.9364 Explore Prob: 0.7980\n",
      "Episode: 119 Total reward: 16.0 Training loss: 784.3796 Explore Prob: 0.7967\n",
      "Episode: 120 Total reward: 30.0 Training loss: 118.9949 Explore Prob: 0.7944\n",
      "Episode: 121 Total reward: 23.0 Training loss: 1015.5011 Explore Prob: 0.7926\n",
      "Episode: 122 Total reward: 14.0 Training loss: 953.0809 Explore Prob: 0.7915\n",
      "Episode: 123 Total reward: 16.0 Training loss: 98.2815 Explore Prob: 0.7902\n",
      "Episode: 124 Total reward: 34.0 Training loss: 963.2944 Explore Prob: 0.7876\n",
      "Episode: 125 Total reward: 19.0 Training loss: 1761.6019 Explore Prob: 0.7861\n",
      "Episode: 126 Total reward: 17.0 Training loss: 123.9881 Explore Prob: 0.7848\n",
      "Episode: 127 Total reward: 8.0 Training loss: 132.6638 Explore Prob: 0.7842\n",
      "Episode: 128 Total reward: 12.0 Training loss: 979.9792 Explore Prob: 0.7833\n",
      "Episode: 129 Total reward: 15.0 Training loss: 160.5679 Explore Prob: 0.7821\n",
      "Episode: 130 Total reward: 16.0 Training loss: 101.4281 Explore Prob: 0.7809\n",
      "Episode: 131 Total reward: 21.0 Training loss: 640.8292 Explore Prob: 0.7792\n",
      "Episode: 132 Total reward: 17.0 Training loss: 317.9124 Explore Prob: 0.7779\n",
      "Episode: 133 Total reward: 12.0 Training loss: 877.9420 Explore Prob: 0.7770\n",
      "Episode: 134 Total reward: 37.0 Training loss: 1970.3043 Explore Prob: 0.7742\n",
      "Episode: 135 Total reward: 12.0 Training loss: 354.2036 Explore Prob: 0.7733\n",
      "Episode: 136 Total reward: 12.0 Training loss: 373.4448 Explore Prob: 0.7723\n",
      "Episode: 137 Total reward: 14.0 Training loss: 104.1487 Explore Prob: 0.7713\n",
      "Episode: 138 Total reward: 22.0 Training loss: 2072.7869 Explore Prob: 0.7696\n",
      "Episode: 139 Total reward: 8.0 Training loss: 111.6492 Explore Prob: 0.7690\n",
      "Episode: 140 Total reward: 12.0 Training loss: 768.7524 Explore Prob: 0.7681\n",
      "Episode: 141 Total reward: 14.0 Training loss: 1527.7133 Explore Prob: 0.7670\n",
      "Episode: 142 Total reward: 44.0 Training loss: 782.2432 Explore Prob: 0.7637\n",
      "Episode: 143 Total reward: 18.0 Training loss: 516.2765 Explore Prob: 0.7624\n",
      "Episode: 144 Total reward: 9.0 Training loss: 1933.5886 Explore Prob: 0.7617\n",
      "Episode: 145 Total reward: 21.0 Training loss: 790.0203 Explore Prob: 0.7601\n",
      "Episode: 146 Total reward: 25.0 Training loss: 103.2123 Explore Prob: 0.7582\n",
      "Episode: 147 Total reward: 18.0 Training loss: 744.7642 Explore Prob: 0.7569\n",
      "Episode: 148 Total reward: 10.0 Training loss: 349.9747 Explore Prob: 0.7561\n",
      "Episode: 149 Total reward: 14.0 Training loss: 1277.0735 Explore Prob: 0.7551\n",
      "Episode: 150 Total reward: 15.0 Training loss: 415.0920 Explore Prob: 0.7540\n",
      "Episode: 151 Total reward: 15.0 Training loss: 2356.6113 Explore Prob: 0.7529\n",
      "Episode: 152 Total reward: 21.0 Training loss: 163.2464 Explore Prob: 0.7513\n",
      "Episode: 153 Total reward: 12.0 Training loss: 81.4606 Explore Prob: 0.7504\n",
      "Episode: 154 Total reward: 53.0 Training loss: 520.2800 Explore Prob: 0.7465\n",
      "Episode: 155 Total reward: 44.0 Training loss: 78.7802 Explore Prob: 0.7433\n",
      "Episode: 156 Total reward: 10.0 Training loss: 779.7050 Explore Prob: 0.7425\n",
      "Episode: 157 Total reward: 12.0 Training loss: 64.6806 Explore Prob: 0.7417\n",
      "Episode: 158 Total reward: 28.0 Training loss: 525.8600 Explore Prob: 0.7396\n",
      "Episode: 159 Total reward: 12.0 Training loss: 1105.7113 Explore Prob: 0.7387\n",
      "Episode: 160 Total reward: 26.0 Training loss: 1627.4015 Explore Prob: 0.7368\n",
      "Episode: 161 Total reward: 23.0 Training loss: 954.4471 Explore Prob: 0.7352\n",
      "Episode: 162 Total reward: 8.0 Training loss: 500.5149 Explore Prob: 0.7346\n",
      "Episode: 163 Total reward: 16.0 Training loss: 94.5126 Explore Prob: 0.7334\n",
      "Episode: 164 Total reward: 24.0 Training loss: 951.3932 Explore Prob: 0.7317\n",
      "Episode: 165 Total reward: 27.0 Training loss: 99.7278 Explore Prob: 0.7298\n",
      "Episode: 166 Total reward: 10.0 Training loss: 1239.4639 Explore Prob: 0.7290\n",
      "Episode: 167 Total reward: 21.0 Training loss: 70.4371 Explore Prob: 0.7275\n",
      "Episode: 168 Total reward: 9.0 Training loss: 73.3383 Explore Prob: 0.7269\n",
      "Episode: 169 Total reward: 8.0 Training loss: 572.2449 Explore Prob: 0.7263\n",
      "Episode: 170 Total reward: 15.0 Training loss: 1369.3586 Explore Prob: 0.7252\n",
      "Episode: 171 Total reward: 23.0 Training loss: 83.1416 Explore Prob: 0.7236\n",
      "Episode: 172 Total reward: 26.0 Training loss: 69.7369 Explore Prob: 0.7217\n",
      "Episode: 173 Total reward: 48.0 Training loss: 44.3665 Explore Prob: 0.7183\n",
      "Episode: 174 Total reward: 28.0 Training loss: 98.2516 Explore Prob: 0.7163\n",
      "Episode: 175 Total reward: 12.0 Training loss: 2891.1440 Explore Prob: 0.7155\n",
      "Episode: 176 Total reward: 15.0 Training loss: 1638.2561 Explore Prob: 0.7144\n",
      "Episode: 177 Total reward: 22.0 Training loss: 1118.3781 Explore Prob: 0.7129\n",
      "Episode: 178 Total reward: 30.0 Training loss: 63.2043 Explore Prob: 0.7108\n",
      "Episode: 179 Total reward: 21.0 Training loss: 2037.0281 Explore Prob: 0.7093\n",
      "Episode: 180 Total reward: 16.0 Training loss: 2245.5730 Explore Prob: 0.7082\n",
      "Episode: 181 Total reward: 8.0 Training loss: 1100.9240 Explore Prob: 0.7076\n",
      "Episode: 182 Total reward: 35.0 Training loss: 571.9130 Explore Prob: 0.7052\n",
      "Episode: 183 Total reward: 14.0 Training loss: 1701.9309 Explore Prob: 0.7042\n",
      "Episode: 184 Total reward: 13.0 Training loss: 1118.5095 Explore Prob: 0.7033\n",
      "Episode: 185 Total reward: 11.0 Training loss: 616.3151 Explore Prob: 0.7026\n",
      "Episode: 186 Total reward: 18.0 Training loss: 570.0713 Explore Prob: 0.7013\n",
      "Episode: 187 Total reward: 12.0 Training loss: 881.7199 Explore Prob: 0.7005\n",
      "Episode: 188 Total reward: 9.0 Training loss: 590.8144 Explore Prob: 0.6999\n",
      "Episode: 189 Total reward: 14.0 Training loss: 51.9561 Explore Prob: 0.6989\n",
      "Episode: 190 Total reward: 16.0 Training loss: 1010.7515 Explore Prob: 0.6978\n",
      "Episode: 191 Total reward: 20.0 Training loss: 1387.2214 Explore Prob: 0.6964\n",
      "Episode: 192 Total reward: 13.0 Training loss: 2491.2981 Explore Prob: 0.6955\n",
      "Episode: 193 Total reward: 27.0 Training loss: 52.4684 Explore Prob: 0.6937\n",
      "Episode: 194 Total reward: 13.0 Training loss: 1260.5481 Explore Prob: 0.6928\n",
      "Episode: 195 Total reward: 14.0 Training loss: 35.1238 Explore Prob: 0.6918\n",
      "Episode: 196 Total reward: 14.0 Training loss: 69.0820 Explore Prob: 0.6909\n",
      "Episode: 197 Total reward: 19.0 Training loss: 703.0967 Explore Prob: 0.6896\n",
      "Episode: 198 Total reward: 20.0 Training loss: 1169.8295 Explore Prob: 0.6882\n",
      "Episode: 199 Total reward: 24.0 Training loss: 825.9564 Explore Prob: 0.6866\n",
      "Episode: 200 Total reward: 15.0 Training loss: 27.4864 Explore Prob: 0.6856\n",
      "Episode: 201 Total reward: 16.0 Training loss: 756.9965 Explore Prob: 0.6845\n",
      "Episode: 202 Total reward: 7.0 Training loss: 32.0059 Explore Prob: 0.6841\n",
      "Episode: 203 Total reward: 11.0 Training loss: 1306.9902 Explore Prob: 0.6833\n",
      "Episode: 204 Total reward: 11.0 Training loss: 2656.1187 Explore Prob: 0.6826\n",
      "Episode: 205 Total reward: 23.0 Training loss: 646.4642 Explore Prob: 0.6810\n",
      "Episode: 206 Total reward: 40.0 Training loss: 26.1913 Explore Prob: 0.6783\n",
      "Episode: 207 Total reward: 12.0 Training loss: 782.1855 Explore Prob: 0.6775\n",
      "Episode: 208 Total reward: 9.0 Training loss: 1387.2137 Explore Prob: 0.6769\n",
      "Episode: 209 Total reward: 12.0 Training loss: 1520.7776 Explore Prob: 0.6761\n",
      "Episode: 210 Total reward: 14.0 Training loss: 1229.1571 Explore Prob: 0.6752\n",
      "Episode: 211 Total reward: 19.0 Training loss: 598.5906 Explore Prob: 0.6739\n",
      "Episode: 212 Total reward: 11.0 Training loss: 2542.1362 Explore Prob: 0.6732\n",
      "Episode: 213 Total reward: 17.0 Training loss: 1307.7283 Explore Prob: 0.6721\n",
      "Episode: 214 Total reward: 15.0 Training loss: 566.3500 Explore Prob: 0.6711\n",
      "Episode: 215 Total reward: 14.0 Training loss: 26.3991 Explore Prob: 0.6702\n",
      "Episode: 216 Total reward: 17.0 Training loss: 10.3903 Explore Prob: 0.6691\n",
      "Episode: 217 Total reward: 24.0 Training loss: 559.9957 Explore Prob: 0.6675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 218 Total reward: 15.0 Training loss: 16.6577 Explore Prob: 0.6665\n",
      "Episode: 219 Total reward: 11.0 Training loss: 536.8705 Explore Prob: 0.6658\n",
      "Episode: 220 Total reward: 11.0 Training loss: 557.0724 Explore Prob: 0.6650\n",
      "Episode: 221 Total reward: 15.0 Training loss: 2003.0137 Explore Prob: 0.6641\n",
      "Episode: 222 Total reward: 13.0 Training loss: 526.4497 Explore Prob: 0.6632\n",
      "Episode: 223 Total reward: 9.0 Training loss: 1348.9749 Explore Prob: 0.6626\n",
      "Episode: 224 Total reward: 14.0 Training loss: 1717.6517 Explore Prob: 0.6617\n",
      "Episode: 225 Total reward: 12.0 Training loss: 14.3408 Explore Prob: 0.6609\n",
      "Episode: 226 Total reward: 19.0 Training loss: 1499.5066 Explore Prob: 0.6597\n",
      "Episode: 227 Total reward: 13.0 Training loss: 1866.4984 Explore Prob: 0.6589\n",
      "Episode: 228 Total reward: 10.0 Training loss: 1010.0218 Explore Prob: 0.6582\n",
      "Episode: 229 Total reward: 14.0 Training loss: 568.6185 Explore Prob: 0.6573\n",
      "Episode: 230 Total reward: 19.0 Training loss: 907.2394 Explore Prob: 0.6561\n",
      "Episode: 231 Total reward: 15.0 Training loss: 566.3493 Explore Prob: 0.6551\n",
      "Episode: 232 Total reward: 15.0 Training loss: 436.4300 Explore Prob: 0.6541\n",
      "Episode: 233 Total reward: 13.0 Training loss: 455.9645 Explore Prob: 0.6533\n",
      "Episode: 234 Total reward: 18.0 Training loss: 1670.6897 Explore Prob: 0.6521\n",
      "Episode: 235 Total reward: 10.0 Training loss: 586.0951 Explore Prob: 0.6515\n",
      "Episode: 236 Total reward: 16.0 Training loss: 420.0207 Explore Prob: 0.6505\n",
      "Episode: 237 Total reward: 15.0 Training loss: 5.7763 Explore Prob: 0.6495\n",
      "Episode: 238 Total reward: 28.0 Training loss: 942.4408 Explore Prob: 0.6477\n",
      "Episode: 239 Total reward: 9.0 Training loss: 514.1175 Explore Prob: 0.6471\n",
      "Episode: 240 Total reward: 15.0 Training loss: 388.8589 Explore Prob: 0.6462\n",
      "Episode: 241 Total reward: 14.0 Training loss: 891.6681 Explore Prob: 0.6453\n",
      "Episode: 242 Total reward: 8.0 Training loss: 18.2945 Explore Prob: 0.6448\n",
      "Episode: 243 Total reward: 10.0 Training loss: 7.1855 Explore Prob: 0.6442\n",
      "Episode: 244 Total reward: 9.0 Training loss: 889.6247 Explore Prob: 0.6436\n",
      "Episode: 245 Total reward: 9.0 Training loss: 378.1281 Explore Prob: 0.6430\n",
      "Episode: 246 Total reward: 13.0 Training loss: 455.0412 Explore Prob: 0.6422\n",
      "Episode: 247 Total reward: 11.0 Training loss: 506.1003 Explore Prob: 0.6415\n",
      "Episode: 248 Total reward: 21.0 Training loss: 490.4905 Explore Prob: 0.6402\n",
      "Episode: 249 Total reward: 12.0 Training loss: 513.2713 Explore Prob: 0.6394\n",
      "Episode: 250 Total reward: 16.0 Training loss: 505.3448 Explore Prob: 0.6384\n",
      "Episode: 251 Total reward: 14.0 Training loss: 1776.5547 Explore Prob: 0.6375\n",
      "Episode: 252 Total reward: 18.0 Training loss: 383.3723 Explore Prob: 0.6364\n",
      "Episode: 253 Total reward: 31.0 Training loss: 332.7837 Explore Prob: 0.6345\n",
      "Episode: 254 Total reward: 13.0 Training loss: 15.4603 Explore Prob: 0.6337\n",
      "Episode: 255 Total reward: 8.0 Training loss: 769.3242 Explore Prob: 0.6332\n",
      "Episode: 256 Total reward: 12.0 Training loss: 603.5413 Explore Prob: 0.6324\n",
      "Episode: 257 Total reward: 7.0 Training loss: 318.7028 Explore Prob: 0.6320\n",
      "Episode: 258 Total reward: 7.0 Training loss: 321.0101 Explore Prob: 0.6315\n",
      "Episode: 259 Total reward: 13.0 Training loss: 348.2799 Explore Prob: 0.6307\n",
      "Episode: 260 Total reward: 21.0 Training loss: 9.6072 Explore Prob: 0.6294\n",
      "Episode: 261 Total reward: 11.0 Training loss: 740.0089 Explore Prob: 0.6288\n",
      "Episode: 262 Total reward: 30.0 Training loss: 349.4105 Explore Prob: 0.6269\n",
      "Episode: 263 Total reward: 11.0 Training loss: 640.4622 Explore Prob: 0.6262\n",
      "Episode: 264 Total reward: 12.0 Training loss: 7.3435 Explore Prob: 0.6255\n",
      "Episode: 265 Total reward: 8.0 Training loss: 276.7721 Explore Prob: 0.6250\n",
      "Episode: 266 Total reward: 10.0 Training loss: 340.7637 Explore Prob: 0.6244\n",
      "Episode: 267 Total reward: 10.0 Training loss: 1362.5554 Explore Prob: 0.6238\n",
      "Episode: 268 Total reward: 14.0 Training loss: 544.5232 Explore Prob: 0.6229\n",
      "Episode: 269 Total reward: 24.0 Training loss: 7.2391 Explore Prob: 0.6214\n",
      "Episode: 270 Total reward: 24.0 Training loss: 763.7769 Explore Prob: 0.6200\n",
      "Episode: 271 Total reward: 15.0 Training loss: 304.5322 Explore Prob: 0.6191\n",
      "Episode: 272 Total reward: 18.0 Training loss: 431.0531 Explore Prob: 0.6180\n",
      "Episode: 273 Total reward: 9.0 Training loss: 876.3253 Explore Prob: 0.6174\n",
      "Episode: 274 Total reward: 14.0 Training loss: 292.7469 Explore Prob: 0.6166\n",
      "Episode: 275 Total reward: 10.0 Training loss: 476.0634 Explore Prob: 0.6160\n",
      "Episode: 276 Total reward: 12.0 Training loss: 700.0922 Explore Prob: 0.6152\n",
      "Episode: 277 Total reward: 12.0 Training loss: 248.7905 Explore Prob: 0.6145\n",
      "Episode: 278 Total reward: 17.0 Training loss: 271.3651 Explore Prob: 0.6135\n",
      "Episode: 279 Total reward: 10.0 Training loss: 268.7800 Explore Prob: 0.6129\n",
      "Episode: 280 Total reward: 8.0 Training loss: 531.1669 Explore Prob: 0.6124\n",
      "Episode: 281 Total reward: 9.0 Training loss: 3.5725 Explore Prob: 0.6118\n",
      "Episode: 282 Total reward: 9.0 Training loss: 251.0882 Explore Prob: 0.6113\n",
      "Episode: 283 Total reward: 10.0 Training loss: 247.4418 Explore Prob: 0.6107\n",
      "Episode: 284 Total reward: 13.0 Training loss: 5.7591 Explore Prob: 0.6099\n",
      "Episode: 285 Total reward: 10.0 Training loss: 724.4579 Explore Prob: 0.6093\n",
      "Episode: 286 Total reward: 15.0 Training loss: 201.9097 Explore Prob: 0.6084\n",
      "Episode: 287 Total reward: 15.0 Training loss: 843.2355 Explore Prob: 0.6075\n",
      "Episode: 288 Total reward: 18.0 Training loss: 764.3175 Explore Prob: 0.6065\n",
      "Episode: 289 Total reward: 11.0 Training loss: 462.6674 Explore Prob: 0.6058\n",
      "Episode: 290 Total reward: 12.0 Training loss: 6.8506 Explore Prob: 0.6051\n",
      "Episode: 291 Total reward: 7.0 Training loss: 437.2437 Explore Prob: 0.6047\n",
      "Episode: 292 Total reward: 7.0 Training loss: 911.6395 Explore Prob: 0.6043\n",
      "Episode: 293 Total reward: 8.0 Training loss: 626.2803 Explore Prob: 0.6038\n",
      "Episode: 294 Total reward: 11.0 Training loss: 252.3704 Explore Prob: 0.6031\n",
      "Episode: 295 Total reward: 12.0 Training loss: 8.1801 Explore Prob: 0.6024\n",
      "Episode: 296 Total reward: 11.0 Training loss: 7.3839 Explore Prob: 0.6018\n",
      "Episode: 297 Total reward: 9.0 Training loss: 253.7516 Explore Prob: 0.6012\n",
      "Episode: 298 Total reward: 9.0 Training loss: 211.4369 Explore Prob: 0.6007\n",
      "Episode: 299 Total reward: 12.0 Training loss: 6.5688 Explore Prob: 0.6000\n",
      "Episode: 300 Total reward: 12.0 Training loss: 218.8342 Explore Prob: 0.5993\n",
      "Episode: 301 Total reward: 12.0 Training loss: 5.2867 Explore Prob: 0.5986\n",
      "Episode: 302 Total reward: 11.0 Training loss: 186.5057 Explore Prob: 0.5979\n",
      "Episode: 303 Total reward: 12.0 Training loss: 694.8412 Explore Prob: 0.5972\n",
      "Episode: 304 Total reward: 14.0 Training loss: 179.9657 Explore Prob: 0.5964\n",
      "Episode: 305 Total reward: 11.0 Training loss: 532.4449 Explore Prob: 0.5958\n",
      "Episode: 306 Total reward: 32.0 Training loss: 658.5790 Explore Prob: 0.5939\n",
      "Episode: 307 Total reward: 11.0 Training loss: 7.5654 Explore Prob: 0.5932\n",
      "Episode: 308 Total reward: 16.0 Training loss: 680.8163 Explore Prob: 0.5923\n",
      "Episode: 309 Total reward: 9.0 Training loss: 188.0383 Explore Prob: 0.5918\n",
      "Episode: 310 Total reward: 35.0 Training loss: 6.8275 Explore Prob: 0.5898\n",
      "Episode: 311 Total reward: 9.0 Training loss: 8.0387 Explore Prob: 0.5892\n",
      "Episode: 312 Total reward: 9.0 Training loss: 325.6945 Explore Prob: 0.5887\n",
      "Episode: 313 Total reward: 9.0 Training loss: 595.9545 Explore Prob: 0.5882\n",
      "Episode: 314 Total reward: 12.0 Training loss: 431.0736 Explore Prob: 0.5875\n",
      "Episode: 315 Total reward: 11.0 Training loss: 172.7766 Explore Prob: 0.5869\n",
      "Episode: 316 Total reward: 14.0 Training loss: 309.2200 Explore Prob: 0.5861\n",
      "Episode: 317 Total reward: 17.0 Training loss: 9.4801 Explore Prob: 0.5851\n",
      "Episode: 318 Total reward: 22.0 Training loss: 13.2521 Explore Prob: 0.5838\n",
      "Episode: 319 Total reward: 22.0 Training loss: 10.8437 Explore Prob: 0.5826\n",
      "Episode: 320 Total reward: 18.0 Training loss: 473.9128 Explore Prob: 0.5815\n",
      "Episode: 321 Total reward: 20.0 Training loss: 426.4207 Explore Prob: 0.5804\n",
      "Episode: 322 Total reward: 9.0 Training loss: 10.7100 Explore Prob: 0.5799\n",
      "Episode: 323 Total reward: 18.0 Training loss: 269.1162 Explore Prob: 0.5788\n",
      "Episode: 324 Total reward: 15.0 Training loss: 112.8971 Explore Prob: 0.5780\n",
      "Episode: 325 Total reward: 14.0 Training loss: 4.8851 Explore Prob: 0.5772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 326 Total reward: 15.0 Training loss: 8.9749 Explore Prob: 0.5763\n",
      "Episode: 327 Total reward: 12.0 Training loss: 10.8010 Explore Prob: 0.5757\n",
      "Episode: 328 Total reward: 21.0 Training loss: 506.7152 Explore Prob: 0.5745\n",
      "Episode: 329 Total reward: 23.0 Training loss: 257.4139 Explore Prob: 0.5732\n",
      "Episode: 330 Total reward: 8.0 Training loss: 779.2489 Explore Prob: 0.5727\n",
      "Episode: 331 Total reward: 17.0 Training loss: 114.0925 Explore Prob: 0.5718\n",
      "Episode: 332 Total reward: 9.0 Training loss: 640.8348 Explore Prob: 0.5713\n",
      "Episode: 333 Total reward: 8.0 Training loss: 10.8398 Explore Prob: 0.5708\n",
      "Episode: 334 Total reward: 15.0 Training loss: 5.9899 Explore Prob: 0.5700\n",
      "Episode: 335 Total reward: 9.0 Training loss: 9.5992 Explore Prob: 0.5695\n",
      "Episode: 336 Total reward: 27.0 Training loss: 352.8935 Explore Prob: 0.5680\n",
      "Episode: 337 Total reward: 19.0 Training loss: 325.7574 Explore Prob: 0.5669\n",
      "Episode: 338 Total reward: 17.0 Training loss: 1089.9818 Explore Prob: 0.5660\n",
      "Episode: 339 Total reward: 26.0 Training loss: 7.2264 Explore Prob: 0.5645\n",
      "Episode: 340 Total reward: 20.0 Training loss: 99.1462 Explore Prob: 0.5634\n",
      "Episode: 341 Total reward: 11.0 Training loss: 112.1094 Explore Prob: 0.5628\n",
      "Episode: 342 Total reward: 10.0 Training loss: 277.6997 Explore Prob: 0.5623\n",
      "Episode: 343 Total reward: 21.0 Training loss: 356.3700 Explore Prob: 0.5611\n",
      "Episode: 344 Total reward: 15.0 Training loss: 9.1917 Explore Prob: 0.5603\n",
      "Episode: 345 Total reward: 9.0 Training loss: 92.8859 Explore Prob: 0.5598\n",
      "Episode: 346 Total reward: 16.0 Training loss: 263.5695 Explore Prob: 0.5589\n",
      "Episode: 347 Total reward: 15.0 Training loss: 82.9574 Explore Prob: 0.5581\n",
      "Episode: 348 Total reward: 8.0 Training loss: 110.2522 Explore Prob: 0.5576\n",
      "Episode: 349 Total reward: 9.0 Training loss: 9.1728 Explore Prob: 0.5571\n",
      "Episode: 350 Total reward: 9.0 Training loss: 183.0283 Explore Prob: 0.5566\n",
      "Episode: 351 Total reward: 12.0 Training loss: 193.0829 Explore Prob: 0.5560\n",
      "Episode: 352 Total reward: 16.0 Training loss: 5.6830 Explore Prob: 0.5551\n",
      "Episode: 353 Total reward: 11.0 Training loss: 252.4744 Explore Prob: 0.5545\n",
      "Episode: 354 Total reward: 25.0 Training loss: 197.1974 Explore Prob: 0.5532\n",
      "Episode: 355 Total reward: 13.0 Training loss: 8.6422 Explore Prob: 0.5525\n",
      "Episode: 356 Total reward: 14.0 Training loss: 101.7333 Explore Prob: 0.5517\n",
      "Episode: 357 Total reward: 12.0 Training loss: 311.8306 Explore Prob: 0.5510\n",
      "Episode: 358 Total reward: 16.0 Training loss: 6.7807 Explore Prob: 0.5502\n",
      "Episode: 359 Total reward: 16.0 Training loss: 169.9594 Explore Prob: 0.5493\n",
      "Episode: 360 Total reward: 17.0 Training loss: 80.5582 Explore Prob: 0.5484\n",
      "Episode: 361 Total reward: 9.0 Training loss: 70.6261 Explore Prob: 0.5479\n",
      "Episode: 362 Total reward: 13.0 Training loss: 372.3335 Explore Prob: 0.5472\n",
      "Episode: 363 Total reward: 13.0 Training loss: 467.0327 Explore Prob: 0.5465\n",
      "Episode: 364 Total reward: 12.0 Training loss: 6.4387 Explore Prob: 0.5459\n",
      "Episode: 365 Total reward: 13.0 Training loss: 148.0922 Explore Prob: 0.5452\n",
      "Episode: 366 Total reward: 13.0 Training loss: 180.1152 Explore Prob: 0.5445\n",
      "Episode: 367 Total reward: 16.0 Training loss: 204.5838 Explore Prob: 0.5436\n",
      "Episode: 368 Total reward: 9.0 Training loss: 507.2357 Explore Prob: 0.5432\n",
      "Episode: 369 Total reward: 22.0 Training loss: 303.8313 Explore Prob: 0.5420\n",
      "Episode: 370 Total reward: 15.0 Training loss: 258.1696 Explore Prob: 0.5412\n",
      "Episode: 371 Total reward: 11.0 Training loss: 67.6034 Explore Prob: 0.5406\n",
      "Episode: 372 Total reward: 13.0 Training loss: 81.5421 Explore Prob: 0.5399\n",
      "Episode: 373 Total reward: 10.0 Training loss: 10.5955 Explore Prob: 0.5394\n",
      "Episode: 374 Total reward: 11.0 Training loss: 84.2552 Explore Prob: 0.5388\n",
      "Episode: 375 Total reward: 12.0 Training loss: 68.7890 Explore Prob: 0.5382\n",
      "Episode: 376 Total reward: 10.0 Training loss: 278.8147 Explore Prob: 0.5376\n",
      "Episode: 377 Total reward: 12.0 Training loss: 262.9601 Explore Prob: 0.5370\n",
      "Episode: 378 Total reward: 19.0 Training loss: 243.0614 Explore Prob: 0.5360\n",
      "Episode: 379 Total reward: 10.0 Training loss: 267.2457 Explore Prob: 0.5355\n",
      "Episode: 380 Total reward: 11.0 Training loss: 358.1369 Explore Prob: 0.5349\n",
      "Episode: 381 Total reward: 10.0 Training loss: 66.4197 Explore Prob: 0.5344\n",
      "Episode: 382 Total reward: 10.0 Training loss: 365.7516 Explore Prob: 0.5338\n",
      "Episode: 383 Total reward: 9.0 Training loss: 381.3547 Explore Prob: 0.5334\n",
      "Episode: 384 Total reward: 20.0 Training loss: 444.0480 Explore Prob: 0.5323\n",
      "Episode: 385 Total reward: 17.0 Training loss: 568.3399 Explore Prob: 0.5314\n",
      "Episode: 386 Total reward: 17.0 Training loss: 6.0118 Explore Prob: 0.5306\n",
      "Episode: 387 Total reward: 15.0 Training loss: 115.1922 Explore Prob: 0.5298\n",
      "Episode: 388 Total reward: 18.0 Training loss: 546.6859 Explore Prob: 0.5288\n",
      "Episode: 389 Total reward: 24.0 Training loss: 10.5542 Explore Prob: 0.5276\n",
      "Episode: 390 Total reward: 21.0 Training loss: 791.4679 Explore Prob: 0.5265\n",
      "Episode: 391 Total reward: 9.0 Training loss: 124.1132 Explore Prob: 0.5261\n",
      "Episode: 392 Total reward: 14.0 Training loss: 3.6385 Explore Prob: 0.5253\n",
      "Episode: 393 Total reward: 11.0 Training loss: 270.0699 Explore Prob: 0.5248\n",
      "Episode: 394 Total reward: 16.0 Training loss: 5.9285 Explore Prob: 0.5239\n",
      "Episode: 395 Total reward: 18.0 Training loss: 99.9537 Explore Prob: 0.5230\n",
      "Episode: 396 Total reward: 25.0 Training loss: 6.1991 Explore Prob: 0.5217\n",
      "Episode: 397 Total reward: 11.0 Training loss: 7.2484 Explore Prob: 0.5212\n",
      "Episode: 398 Total reward: 15.0 Training loss: 516.0544 Explore Prob: 0.5204\n",
      "Episode: 399 Total reward: 12.0 Training loss: 227.5853 Explore Prob: 0.5198\n",
      "Episode: 400 Total reward: 10.0 Training loss: 260.4976 Explore Prob: 0.5193\n",
      "Episode: 401 Total reward: 16.0 Training loss: 170.1245 Explore Prob: 0.5185\n",
      "Episode: 402 Total reward: 12.0 Training loss: 14.7213 Explore Prob: 0.5179\n",
      "Episode: 403 Total reward: 18.0 Training loss: 58.0685 Explore Prob: 0.5169\n",
      "Episode: 404 Total reward: 12.0 Training loss: 5.2465 Explore Prob: 0.5163\n",
      "Episode: 405 Total reward: 8.0 Training loss: 7.7703 Explore Prob: 0.5159\n",
      "Episode: 406 Total reward: 9.0 Training loss: 7.5741 Explore Prob: 0.5155\n",
      "Episode: 407 Total reward: 10.0 Training loss: 224.5236 Explore Prob: 0.5150\n",
      "Episode: 408 Total reward: 13.0 Training loss: 173.8136 Explore Prob: 0.5143\n",
      "Episode: 409 Total reward: 29.0 Training loss: 57.6048 Explore Prob: 0.5129\n",
      "Episode: 410 Total reward: 10.0 Training loss: 8.1059 Explore Prob: 0.5124\n",
      "Episode: 411 Total reward: 14.0 Training loss: 7.2130 Explore Prob: 0.5117\n",
      "Episode: 412 Total reward: 20.0 Training loss: 7.2710 Explore Prob: 0.5106\n",
      "Episode: 413 Total reward: 12.0 Training loss: 89.7666 Explore Prob: 0.5100\n",
      "Episode: 414 Total reward: 11.0 Training loss: 211.7868 Explore Prob: 0.5095\n",
      "Episode: 415 Total reward: 7.0 Training loss: 7.2208 Explore Prob: 0.5091\n",
      "Episode: 416 Total reward: 12.0 Training loss: 241.6388 Explore Prob: 0.5086\n",
      "Episode: 417 Total reward: 9.0 Training loss: 6.1768 Explore Prob: 0.5081\n",
      "Episode: 418 Total reward: 10.0 Training loss: 333.1497 Explore Prob: 0.5076\n",
      "Episode: 419 Total reward: 12.0 Training loss: 8.5297 Explore Prob: 0.5070\n",
      "Episode: 420 Total reward: 12.0 Training loss: 133.7420 Explore Prob: 0.5064\n",
      "Episode: 421 Total reward: 13.0 Training loss: 48.7803 Explore Prob: 0.5058\n",
      "Episode: 422 Total reward: 8.0 Training loss: 226.7520 Explore Prob: 0.5054\n",
      "Episode: 423 Total reward: 38.0 Training loss: 6.9810 Explore Prob: 0.5035\n",
      "Episode: 424 Total reward: 14.0 Training loss: 232.7470 Explore Prob: 0.5028\n",
      "Episode: 425 Total reward: 10.0 Training loss: 454.8154 Explore Prob: 0.5023\n",
      "Episode: 426 Total reward: 12.0 Training loss: 378.1195 Explore Prob: 0.5017\n",
      "Episode: 427 Total reward: 21.0 Training loss: 39.5405 Explore Prob: 0.5007\n",
      "Episode: 428 Total reward: 17.0 Training loss: 5.0810 Explore Prob: 0.4999\n",
      "Episode: 429 Total reward: 22.0 Training loss: 472.3914 Explore Prob: 0.4988\n",
      "Episode: 430 Total reward: 14.0 Training loss: 126.4150 Explore Prob: 0.4981\n",
      "Episode: 431 Total reward: 15.0 Training loss: 87.7486 Explore Prob: 0.4974\n",
      "Episode: 432 Total reward: 13.0 Training loss: 270.3234 Explore Prob: 0.4967\n",
      "Episode: 433 Total reward: 11.0 Training loss: 41.0740 Explore Prob: 0.4962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 434 Total reward: 10.0 Training loss: 2.7699 Explore Prob: 0.4957\n",
      "Episode: 435 Total reward: 12.0 Training loss: 266.0764 Explore Prob: 0.4951\n",
      "Episode: 436 Total reward: 7.0 Training loss: 7.5234 Explore Prob: 0.4948\n",
      "Episode: 437 Total reward: 17.0 Training loss: 65.9069 Explore Prob: 0.4940\n",
      "Episode: 438 Total reward: 22.0 Training loss: 310.5612 Explore Prob: 0.4929\n",
      "Episode: 439 Total reward: 12.0 Training loss: 52.6312 Explore Prob: 0.4923\n",
      "Episode: 440 Total reward: 11.0 Training loss: 120.1447 Explore Prob: 0.4918\n",
      "Episode: 441 Total reward: 14.0 Training loss: 167.7856 Explore Prob: 0.4911\n",
      "Episode: 442 Total reward: 19.0 Training loss: 2.6016 Explore Prob: 0.4902\n",
      "Episode: 443 Total reward: 15.0 Training loss: 179.0249 Explore Prob: 0.4895\n",
      "Episode: 444 Total reward: 17.0 Training loss: 35.5256 Explore Prob: 0.4887\n",
      "Episode: 445 Total reward: 14.0 Training loss: 2.9116 Explore Prob: 0.4880\n",
      "Episode: 446 Total reward: 18.0 Training loss: 112.0266 Explore Prob: 0.4871\n",
      "Episode: 447 Total reward: 10.0 Training loss: 75.8133 Explore Prob: 0.4867\n",
      "Episode: 448 Total reward: 13.0 Training loss: 7.3706 Explore Prob: 0.4860\n",
      "Episode: 449 Total reward: 10.0 Training loss: 64.5540 Explore Prob: 0.4856\n",
      "Episode: 450 Total reward: 22.0 Training loss: 82.6737 Explore Prob: 0.4845\n",
      "Episode: 451 Total reward: 12.0 Training loss: 33.6136 Explore Prob: 0.4840\n",
      "Episode: 452 Total reward: 8.0 Training loss: 156.0205 Explore Prob: 0.4836\n",
      "Episode: 453 Total reward: 19.0 Training loss: 2.6161 Explore Prob: 0.4827\n",
      "Episode: 454 Total reward: 9.0 Training loss: 94.7202 Explore Prob: 0.4822\n",
      "Episode: 455 Total reward: 9.0 Training loss: 3.1265 Explore Prob: 0.4818\n",
      "Episode: 456 Total reward: 25.0 Training loss: 208.4206 Explore Prob: 0.4806\n",
      "Episode: 457 Total reward: 9.0 Training loss: 110.0592 Explore Prob: 0.4802\n",
      "Episode: 458 Total reward: 10.0 Training loss: 363.7721 Explore Prob: 0.4798\n",
      "Episode: 459 Total reward: 9.0 Training loss: 104.4777 Explore Prob: 0.4793\n",
      "Episode: 460 Total reward: 15.0 Training loss: 152.9834 Explore Prob: 0.4786\n",
      "Episode: 461 Total reward: 17.0 Training loss: 4.2286 Explore Prob: 0.4778\n",
      "Episode: 462 Total reward: 19.0 Training loss: 102.6283 Explore Prob: 0.4769\n",
      "Episode: 463 Total reward: 10.0 Training loss: 65.4757 Explore Prob: 0.4765\n",
      "Episode: 464 Total reward: 18.0 Training loss: 119.4054 Explore Prob: 0.4756\n",
      "Episode: 465 Total reward: 11.0 Training loss: 1.6677 Explore Prob: 0.4751\n",
      "Episode: 466 Total reward: 16.0 Training loss: 126.2259 Explore Prob: 0.4744\n",
      "Episode: 467 Total reward: 9.0 Training loss: 252.2102 Explore Prob: 0.4740\n",
      "Episode: 468 Total reward: 12.0 Training loss: 40.2184 Explore Prob: 0.4734\n",
      "Episode: 469 Total reward: 13.0 Training loss: 3.0152 Explore Prob: 0.4728\n",
      "Episode: 470 Total reward: 13.0 Training loss: 25.7694 Explore Prob: 0.4722\n",
      "Episode: 471 Total reward: 9.0 Training loss: 3.8858 Explore Prob: 0.4718\n",
      "Episode: 472 Total reward: 15.0 Training loss: 2.3142 Explore Prob: 0.4711\n",
      "Episode: 473 Total reward: 15.0 Training loss: 84.1554 Explore Prob: 0.4704\n",
      "Episode: 474 Total reward: 8.0 Training loss: 81.9370 Explore Prob: 0.4700\n",
      "Episode: 475 Total reward: 21.0 Training loss: 220.8074 Explore Prob: 0.4691\n",
      "Episode: 476 Total reward: 10.0 Training loss: 199.0962 Explore Prob: 0.4686\n",
      "Episode: 477 Total reward: 24.0 Training loss: 62.0595 Explore Prob: 0.4675\n",
      "Episode: 478 Total reward: 17.0 Training loss: 29.3710 Explore Prob: 0.4667\n",
      "Episode: 479 Total reward: 11.0 Training loss: 196.5266 Explore Prob: 0.4662\n",
      "Episode: 480 Total reward: 8.0 Training loss: 130.4843 Explore Prob: 0.4659\n",
      "Episode: 481 Total reward: 19.0 Training loss: 103.3760 Explore Prob: 0.4650\n",
      "Episode: 482 Total reward: 15.0 Training loss: 3.9794 Explore Prob: 0.4643\n",
      "Episode: 483 Total reward: 12.0 Training loss: 65.2064 Explore Prob: 0.4638\n",
      "Episode: 484 Total reward: 15.0 Training loss: 144.1650 Explore Prob: 0.4631\n",
      "Episode: 485 Total reward: 12.0 Training loss: 62.7093 Explore Prob: 0.4626\n",
      "Episode: 486 Total reward: 10.0 Training loss: 2.3740 Explore Prob: 0.4621\n",
      "Episode: 487 Total reward: 10.0 Training loss: 117.3114 Explore Prob: 0.4616\n",
      "Episode: 488 Total reward: 13.0 Training loss: 1.0651 Explore Prob: 0.4611\n",
      "Episode: 489 Total reward: 13.0 Training loss: 124.9893 Explore Prob: 0.4605\n",
      "Episode: 490 Total reward: 11.0 Training loss: 56.4503 Explore Prob: 0.4600\n",
      "Episode: 491 Total reward: 14.0 Training loss: 55.9799 Explore Prob: 0.4594\n",
      "Episode: 492 Total reward: 17.0 Training loss: 2.7908 Explore Prob: 0.4586\n",
      "Episode: 493 Total reward: 20.0 Training loss: 20.9658 Explore Prob: 0.4577\n",
      "Episode: 494 Total reward: 14.0 Training loss: 2.3477 Explore Prob: 0.4571\n",
      "Episode: 495 Total reward: 11.0 Training loss: 1.9756 Explore Prob: 0.4566\n",
      "Episode: 496 Total reward: 12.0 Training loss: 66.0889 Explore Prob: 0.4560\n",
      "Episode: 497 Total reward: 18.0 Training loss: 19.3504 Explore Prob: 0.4552\n",
      "Episode: 498 Total reward: 18.0 Training loss: 46.9825 Explore Prob: 0.4544\n",
      "Episode: 499 Total reward: 29.0 Training loss: 53.0866 Explore Prob: 0.4531\n",
      "Episode: 500 Total reward: 24.0 Training loss: 1.5738 Explore Prob: 0.4521\n",
      "Episode: 501 Total reward: 30.0 Training loss: 1.6276 Explore Prob: 0.4508\n",
      "Episode: 502 Total reward: 40.0 Training loss: 119.2110 Explore Prob: 0.4490\n",
      "Episode: 503 Total reward: 41.0 Training loss: 37.3064 Explore Prob: 0.4472\n",
      "Episode: 504 Total reward: 31.0 Training loss: 24.3709 Explore Prob: 0.4459\n",
      "Episode: 505 Total reward: 21.0 Training loss: 34.5752 Explore Prob: 0.4449\n",
      "Episode: 506 Total reward: 27.0 Training loss: 1.9415 Explore Prob: 0.4438\n",
      "Episode: 507 Total reward: 17.0 Training loss: 70.5255 Explore Prob: 0.4430\n",
      "Episode: 508 Total reward: 27.0 Training loss: 1.8364 Explore Prob: 0.4419\n",
      "Episode: 509 Total reward: 79.0 Training loss: 1.4003 Explore Prob: 0.4385\n",
      "Episode: 510 Total reward: 52.0 Training loss: 42.5839 Explore Prob: 0.4362\n",
      "Episode: 511 Total reward: 11.0 Training loss: 88.1833 Explore Prob: 0.4358\n",
      "Episode: 512 Total reward: 60.0 Training loss: 29.0180 Explore Prob: 0.4332\n",
      "Episode: 513 Total reward: 35.0 Training loss: 83.4141 Explore Prob: 0.4317\n",
      "Episode: 514 Total reward: 19.0 Training loss: 56.5151 Explore Prob: 0.4309\n",
      "Episode: 515 Total reward: 42.0 Training loss: 26.8731 Explore Prob: 0.4292\n",
      "Episode: 516 Total reward: 86.0 Training loss: 22.2484 Explore Prob: 0.4256\n",
      "Episode: 517 Total reward: 100.0 Training loss: 19.0845 Explore Prob: 0.4215\n",
      "Episode: 518 Total reward: 63.0 Training loss: 12.7853 Explore Prob: 0.4189\n",
      "Episode: 519 Total reward: 50.0 Training loss: 15.4371 Explore Prob: 0.4168\n",
      "Episode: 520 Total reward: 94.0 Training loss: 1.0937 Explore Prob: 0.4130\n",
      "Episode: 521 Total reward: 54.0 Training loss: 33.9327 Explore Prob: 0.4109\n",
      "Episode: 522 Total reward: 41.0 Training loss: 55.7634 Explore Prob: 0.4092\n",
      "Episode: 523 Total reward: 52.0 Training loss: 1.2507 Explore Prob: 0.4071\n",
      "Episode: 524 Total reward: 39.0 Training loss: 23.4224 Explore Prob: 0.4056\n",
      "Episode: 525 Total reward: 43.0 Training loss: 14.1365 Explore Prob: 0.4039\n",
      "Episode: 526 Total reward: 51.0 Training loss: 2.4817 Explore Prob: 0.4019\n",
      "Episode: 527 Total reward: 105.0 Training loss: 16.8578 Explore Prob: 0.3978\n",
      "Episode: 528 Total reward: 58.0 Training loss: 2.1125 Explore Prob: 0.3956\n",
      "Episode: 529 Total reward: 59.0 Training loss: 15.2409 Explore Prob: 0.3933\n",
      "Episode: 530 Total reward: 36.0 Training loss: 90.9817 Explore Prob: 0.3919\n",
      "Episode: 531 Total reward: 86.0 Training loss: 2.0389 Explore Prob: 0.3886\n",
      "Episode: 532 Total reward: 22.0 Training loss: 1.9064 Explore Prob: 0.3878\n",
      "Episode: 533 Total reward: 99.0 Training loss: 11.0040 Explore Prob: 0.3841\n",
      "Episode: 534 Total reward: 101.0 Training loss: 19.8485 Explore Prob: 0.3803\n",
      "Episode: 535 Total reward: 13.0 Training loss: 28.2151 Explore Prob: 0.3799\n",
      "Episode: 536 Total reward: 65.0 Training loss: 33.9799 Explore Prob: 0.3775\n",
      "Episode: 537 Total reward: 46.0 Training loss: 18.7011 Explore Prob: 0.3758\n",
      "Episode: 538 Total reward: 89.0 Training loss: 24.6719 Explore Prob: 0.3725\n",
      "Episode: 539 Total reward: 73.0 Training loss: 45.3222 Explore Prob: 0.3699\n",
      "Episode: 540 Total reward: 57.0 Training loss: 48.0511 Explore Prob: 0.3678\n",
      "Episode: 541 Total reward: 72.0 Training loss: 16.0973 Explore Prob: 0.3653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 542 Total reward: 117.0 Training loss: 23.7003 Explore Prob: 0.3611\n",
      "Episode: 543 Total reward: 28.0 Training loss: 40.8575 Explore Prob: 0.3602\n",
      "Episode: 544 Total reward: 76.0 Training loss: 2.9313 Explore Prob: 0.3575\n",
      "Episode: 545 Total reward: 101.0 Training loss: 7.9095 Explore Prob: 0.3540\n",
      "Episode: 546 Total reward: 113.0 Training loss: 13.8029 Explore Prob: 0.3502\n",
      "Episode: 547 Total reward: 66.0 Training loss: 3.0996 Explore Prob: 0.3479\n",
      "Episode: 548 Total reward: 67.0 Training loss: 3.2105 Explore Prob: 0.3457\n",
      "Episode: 549 Total reward: 40.0 Training loss: 13.0362 Explore Prob: 0.3443\n",
      "Episode: 550 Total reward: 61.0 Training loss: 47.1425 Explore Prob: 0.3423\n",
      "Episode: 551 Total reward: 53.0 Training loss: 34.4108 Explore Prob: 0.3405\n",
      "Episode: 552 Total reward: 88.0 Training loss: 33.9380 Explore Prob: 0.3376\n",
      "Episode: 553 Total reward: 113.0 Training loss: 2.7340 Explore Prob: 0.3340\n",
      "Episode: 554 Total reward: 96.0 Training loss: 99.0168 Explore Prob: 0.3309\n",
      "Episode: 555 Total reward: 82.0 Training loss: 74.5970 Explore Prob: 0.3282\n",
      "Episode: 556 Total reward: 87.0 Training loss: 90.3223 Explore Prob: 0.3255\n",
      "Episode: 557 Total reward: 67.0 Training loss: 2.1060 Explore Prob: 0.3234\n",
      "Episode: 558 Total reward: 47.0 Training loss: 16.2416 Explore Prob: 0.3219\n",
      "Episode: 559 Total reward: 43.0 Training loss: 2.8063 Explore Prob: 0.3206\n",
      "Episode: 560 Total reward: 36.0 Training loss: 95.7392 Explore Prob: 0.3195\n",
      "Episode: 561 Total reward: 84.0 Training loss: 70.0911 Explore Prob: 0.3169\n",
      "Episode: 562 Total reward: 28.0 Training loss: 18.3394 Explore Prob: 0.3160\n",
      "Episode: 563 Total reward: 67.0 Training loss: 20.8313 Explore Prob: 0.3140\n",
      "Episode: 564 Total reward: 78.0 Training loss: 3.9129 Explore Prob: 0.3116\n",
      "Episode: 565 Total reward: 62.0 Training loss: 2.6367 Explore Prob: 0.3097\n",
      "Episode: 566 Total reward: 81.0 Training loss: 18.6221 Explore Prob: 0.3073\n",
      "Episode: 567 Total reward: 54.0 Training loss: 8.5094 Explore Prob: 0.3057\n",
      "Episode: 568 Total reward: 108.0 Training loss: 84.8588 Explore Prob: 0.3025\n",
      "Episode: 569 Total reward: 68.0 Training loss: 14.5373 Explore Prob: 0.3006\n",
      "Episode: 570 Total reward: 79.0 Training loss: 3.9780 Explore Prob: 0.2983\n",
      "Episode: 571 Total reward: 45.0 Training loss: 94.9014 Explore Prob: 0.2970\n",
      "Episode: 572 Total reward: 62.0 Training loss: 2.2508 Explore Prob: 0.2952\n",
      "Episode: 573 Total reward: 43.0 Training loss: 3.2068 Explore Prob: 0.2940\n",
      "Episode: 574 Total reward: 45.0 Training loss: 3.2966 Explore Prob: 0.2927\n",
      "Episode: 575 Total reward: 29.0 Training loss: 3.4226 Explore Prob: 0.2919\n",
      "Episode: 576 Total reward: 59.0 Training loss: 3.2328 Explore Prob: 0.2902\n",
      "Episode: 577 Total reward: 67.0 Training loss: 8.1718 Explore Prob: 0.2884\n",
      "Episode: 578 Total reward: 56.0 Training loss: 4.8461 Explore Prob: 0.2868\n",
      "Episode: 579 Total reward: 47.0 Training loss: 10.9965 Explore Prob: 0.2855\n",
      "Episode: 580 Total reward: 101.0 Training loss: 1.5384 Explore Prob: 0.2827\n",
      "Episode: 581 Total reward: 55.0 Training loss: 15.1546 Explore Prob: 0.2812\n",
      "Episode: 582 Total reward: 66.0 Training loss: 120.8991 Explore Prob: 0.2795\n",
      "Episode: 583 Total reward: 122.0 Training loss: 1.7043 Explore Prob: 0.2762\n",
      "Episode: 584 Total reward: 62.0 Training loss: 32.1374 Explore Prob: 0.2745\n",
      "Episode: 585 Total reward: 97.0 Training loss: 99.3388 Explore Prob: 0.2720\n",
      "Episode: 586 Total reward: 40.0 Training loss: 34.8620 Explore Prob: 0.2709\n",
      "Episode: 587 Total reward: 73.0 Training loss: 116.4908 Explore Prob: 0.2690\n",
      "Episode: 588 Total reward: 122.0 Training loss: 76.7548 Explore Prob: 0.2659\n",
      "Episode: 589 Total reward: 53.0 Training loss: 10.6969 Explore Prob: 0.2646\n",
      "Episode: 590 Total reward: 82.0 Training loss: 118.1081 Explore Prob: 0.2625\n",
      "Episode: 591 Total reward: 59.0 Training loss: 107.6526 Explore Prob: 0.2610\n",
      "Episode: 592 Total reward: 56.0 Training loss: 3.7407 Explore Prob: 0.2596\n",
      "Episode: 593 Total reward: 196.0 Training loss: 28.7926 Explore Prob: 0.2547\n",
      "Episode: 594 Total reward: 42.0 Training loss: 2.3430 Explore Prob: 0.2537\n",
      "Episode: 595 Total reward: 116.0 Training loss: 12.7318 Explore Prob: 0.2509\n",
      "Episode: 596 Total reward: 95.0 Training loss: 50.2053 Explore Prob: 0.2486\n",
      "Episode: 597 Total reward: 59.0 Training loss: 58.8130 Explore Prob: 0.2472\n",
      "Episode: 598 Total reward: 46.0 Training loss: 2.2514 Explore Prob: 0.2461\n",
      "Episode: 599 Total reward: 59.0 Training loss: 2.7280 Explore Prob: 0.2447\n",
      "Episode: 600 Total reward: 71.0 Training loss: 2.2160 Explore Prob: 0.2431\n",
      "Episode: 601 Total reward: 111.0 Training loss: 5.5299 Explore Prob: 0.2405\n",
      "Episode: 602 Total reward: 120.0 Training loss: 17.0880 Explore Prob: 0.2378\n",
      "Episode: 603 Total reward: 71.0 Training loss: 1.3104 Explore Prob: 0.2362\n",
      "Episode: 604 Total reward: 63.0 Training loss: 1.1860 Explore Prob: 0.2347\n",
      "Episode: 605 Total reward: 73.0 Training loss: 126.9340 Explore Prob: 0.2331\n",
      "Episode: 606 Total reward: 58.0 Training loss: 38.9069 Explore Prob: 0.2318\n",
      "Episode: 607 Total reward: 102.0 Training loss: 4.7810 Explore Prob: 0.2296\n",
      "Episode: 608 Total reward: 64.0 Training loss: 2.7537 Explore Prob: 0.2282\n",
      "Episode: 609 Total reward: 95.0 Training loss: 126.9105 Explore Prob: 0.2261\n",
      "Episode: 610 Total reward: 56.0 Training loss: 7.9942 Explore Prob: 0.2249\n",
      "Episode: 611 Total reward: 199.0 Training loss: 3.3714 Explore Prob: 0.2207\n",
      "Episode: 612 Total reward: 81.0 Training loss: 1.9542 Explore Prob: 0.2190\n",
      "Episode: 613 Total reward: 79.0 Training loss: 1.9387 Explore Prob: 0.2173\n",
      "Episode: 614 Total reward: 67.0 Training loss: 30.5134 Explore Prob: 0.2159\n",
      "Episode: 615 Total reward: 72.0 Training loss: 20.3565 Explore Prob: 0.2144\n",
      "Episode: 616 Total reward: 167.0 Training loss: 2.3431 Explore Prob: 0.2111\n",
      "Episode: 617 Total reward: 140.0 Training loss: 159.4478 Explore Prob: 0.2083\n",
      "Episode: 618 Total reward: 117.0 Training loss: 38.6254 Explore Prob: 0.2060\n",
      "Episode: 619 Total reward: 105.0 Training loss: 2.1773 Explore Prob: 0.2039\n",
      "Episode: 620 Total reward: 47.0 Training loss: 4.3730 Explore Prob: 0.2030\n",
      "Episode: 621 Total reward: 153.0 Training loss: 3.0065 Explore Prob: 0.2001\n",
      "Episode: 622 Total reward: 194.0 Training loss: 1.3948 Explore Prob: 0.1964\n",
      "Episode: 623 Total reward: 134.0 Training loss: 0.6403 Explore Prob: 0.1939\n",
      "Episode: 624 Total reward: 92.0 Training loss: 1.1438 Explore Prob: 0.1923\n",
      "Episode: 625 Total reward: 116.0 Training loss: 3.1875 Explore Prob: 0.1902\n",
      "Episode: 626 Total reward: 101.0 Training loss: 1.2323 Explore Prob: 0.1883\n",
      "Episode: 627 Total reward: 70.0 Training loss: 2.3806 Explore Prob: 0.1871\n",
      "Episode: 628 Total reward: 179.0 Training loss: 121.4228 Explore Prob: 0.1840\n",
      "Episode: 629 Total reward: 137.0 Training loss: 161.5732 Explore Prob: 0.1816\n",
      "Episode: 630 Total reward: 101.0 Training loss: 51.3766 Explore Prob: 0.1799\n",
      "Episode: 631 Total reward: 90.0 Training loss: 0.8029 Explore Prob: 0.1783\n",
      "Episode: 632 Total reward: 78.0 Training loss: 95.7600 Explore Prob: 0.1770\n",
      "Episode: 633 Total reward: 103.0 Training loss: 2.1527 Explore Prob: 0.1753\n",
      "Episode: 634 Total reward: 85.0 Training loss: 2.1375 Explore Prob: 0.1739\n",
      "Episode: 635 Total reward: 73.0 Training loss: 1.7536 Explore Prob: 0.1727\n",
      "Episode: 636 Total reward: 56.0 Training loss: 1.0499 Explore Prob: 0.1718\n",
      "Episode: 637 Total reward: 100.0 Training loss: 0.7432 Explore Prob: 0.1702\n",
      "Episode: 638 Total reward: 135.0 Training loss: 1.2272 Explore Prob: 0.1681\n",
      "Episode: 639 Total reward: 162.0 Training loss: 1.8167 Explore Prob: 0.1655\n",
      "Episode: 640 Total reward: 112.0 Training loss: 1.8980 Explore Prob: 0.1638\n",
      "Episode: 641 Total reward: 74.0 Training loss: 1.0716 Explore Prob: 0.1627\n",
      "Episode: 642 Total reward: 133.0 Training loss: 1.6934 Explore Prob: 0.1606\n",
      "Episode: 643 Total reward: 199.0 Training loss: 0.8929 Explore Prob: 0.1577\n",
      "Episode: 644 Total reward: 199.0 Training loss: 0.7954 Explore Prob: 0.1548\n",
      "Episode: 645 Total reward: 199.0 Training loss: 92.0424 Explore Prob: 0.1519\n",
      "Episode: 646 Total reward: 134.0 Training loss: 1.0658 Explore Prob: 0.1500\n",
      "Episode: 647 Total reward: 75.0 Training loss: 1.0928 Explore Prob: 0.1490\n",
      "Episode: 648 Total reward: 199.0 Training loss: 0.9901 Explore Prob: 0.1462\n",
      "Episode: 649 Total reward: 177.0 Training loss: 0.7371 Explore Prob: 0.1438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 650 Total reward: 199.0 Training loss: 1.8125 Explore Prob: 0.1412\n",
      "Episode: 651 Total reward: 87.0 Training loss: 1.5656 Explore Prob: 0.1401\n",
      "Episode: 652 Total reward: 199.0 Training loss: 1.0751 Explore Prob: 0.1375\n",
      "Episode: 653 Total reward: 199.0 Training loss: 0.3895 Explore Prob: 0.1350\n",
      "Episode: 654 Total reward: 199.0 Training loss: 0.8086 Explore Prob: 0.1325\n",
      "Episode: 655 Total reward: 147.0 Training loss: 0.9241 Explore Prob: 0.1307\n",
      "Episode: 656 Total reward: 159.0 Training loss: 1.6602 Explore Prob: 0.1288\n",
      "Episode: 657 Total reward: 199.0 Training loss: 2.1363 Explore Prob: 0.1265\n",
      "Episode: 658 Total reward: 199.0 Training loss: 0.7451 Explore Prob: 0.1242\n",
      "Episode: 659 Total reward: 199.0 Training loss: 0.5161 Explore Prob: 0.1220\n",
      "Episode: 660 Total reward: 187.0 Training loss: 0.8356 Explore Prob: 0.1199\n",
      "Episode: 661 Total reward: 199.0 Training loss: 1.2196 Explore Prob: 0.1177\n",
      "Episode: 662 Total reward: 199.0 Training loss: 0.8289 Explore Prob: 0.1156\n",
      "Episode: 663 Total reward: 199.0 Training loss: 0.9841 Explore Prob: 0.1135\n",
      "Episode: 664 Total reward: 199.0 Training loss: 1.6126 Explore Prob: 0.1115\n",
      "Episode: 665 Total reward: 199.0 Training loss: 1.2587 Explore Prob: 0.1095\n",
      "Episode: 666 Total reward: 199.0 Training loss: 113.3333 Explore Prob: 0.1075\n",
      "Episode: 667 Total reward: 199.0 Training loss: 0.9209 Explore Prob: 0.1056\n",
      "Episode: 668 Total reward: 199.0 Training loss: 0.7855 Explore Prob: 0.1037\n",
      "Episode: 669 Total reward: 199.0 Training loss: 0.4351 Explore Prob: 0.1019\n",
      "Episode: 670 Total reward: 199.0 Training loss: 166.5029 Explore Prob: 0.1001\n",
      "Episode: 671 Total reward: 199.0 Training loss: 1.6818 Explore Prob: 0.0983\n",
      "Episode: 672 Total reward: 199.0 Training loss: 0.5784 Explore Prob: 0.0965\n",
      "Episode: 673 Total reward: 199.0 Training loss: 0.7180 Explore Prob: 0.0948\n",
      "Episode: 674 Total reward: 199.0 Training loss: 0.4169 Explore Prob: 0.0932\n",
      "Episode: 675 Total reward: 199.0 Training loss: 0.4754 Explore Prob: 0.0915\n",
      "Episode: 676 Total reward: 199.0 Training loss: 0.9634 Explore Prob: 0.0899\n",
      "Episode: 677 Total reward: 199.0 Training loss: 0.5736 Explore Prob: 0.0883\n",
      "Episode: 678 Total reward: 199.0 Training loss: 0.4449 Explore Prob: 0.0868\n",
      "Episode: 679 Total reward: 199.0 Training loss: 0.8022 Explore Prob: 0.0853\n",
      "Episode: 680 Total reward: 199.0 Training loss: 0.7936 Explore Prob: 0.0838\n",
      "Episode: 681 Total reward: 199.0 Training loss: 0.7528 Explore Prob: 0.0823\n",
      "Episode: 682 Total reward: 199.0 Training loss: 0.7919 Explore Prob: 0.0809\n",
      "Episode: 683 Total reward: 199.0 Training loss: 0.4148 Explore Prob: 0.0795\n",
      "Episode: 684 Total reward: 199.0 Training loss: 0.8018 Explore Prob: 0.0782\n",
      "Episode: 685 Total reward: 199.0 Training loss: 0.7528 Explore Prob: 0.0768\n",
      "Episode: 686 Total reward: 199.0 Training loss: 0.3577 Explore Prob: 0.0755\n",
      "Episode: 687 Total reward: 199.0 Training loss: 0.4341 Explore Prob: 0.0742\n",
      "Episode: 688 Total reward: 199.0 Training loss: 0.5618 Explore Prob: 0.0729\n",
      "Episode: 689 Total reward: 199.0 Training loss: 0.6256 Explore Prob: 0.0717\n",
      "Episode: 690 Total reward: 199.0 Training loss: 0.3183 Explore Prob: 0.0705\n",
      "Episode: 691 Total reward: 199.0 Training loss: 0.3586 Explore Prob: 0.0693\n",
      "Episode: 692 Total reward: 199.0 Training loss: 0.4725 Explore Prob: 0.0681\n",
      "Episode: 693 Total reward: 199.0 Training loss: 0.4117 Explore Prob: 0.0670\n",
      "Episode: 694 Total reward: 199.0 Training loss: 0.3163 Explore Prob: 0.0659\n",
      "Episode: 695 Total reward: 199.0 Training loss: 0.5751 Explore Prob: 0.0648\n",
      "Episode: 696 Total reward: 199.0 Training loss: 0.2472 Explore Prob: 0.0637\n",
      "Episode: 697 Total reward: 199.0 Training loss: 0.3796 Explore Prob: 0.0626\n",
      "Episode: 698 Total reward: 199.0 Training loss: 0.5253 Explore Prob: 0.0616\n",
      "Episode: 699 Total reward: 199.0 Training loss: 0.3536 Explore Prob: 0.0606\n"
     ]
    }
   ],
   "source": [
    "# Now train with experience\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "rewards_list = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    decay_step = 0\n",
    "    for episode in range(1, train_episodes):\n",
    "        total_reward = 0\n",
    "        step =0\n",
    "        while step < max_steps:\n",
    "            decay_step +=1\n",
    "            env.render()\n",
    "            \n",
    "            # Explore or Exploit\n",
    "            explore_prob = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*decay_step)\n",
    "            \n",
    "            if explore_prob > np.random.rand() : # return [0,1)\n",
    "                # random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Get action from Q-network\n",
    "                feed = {mainQN.inputs_:state.reshape((1, *state.shape))}\n",
    "                Q_pred = sess.run(mainQN.output , feed_dict= feed)\n",
    "                action = np.argmax(Q_pred)\n",
    "                \n",
    "            # Take action, get new_state and reward\n",
    "            next_state , reward , done, _ = env.step(action)\n",
    "            \n",
    "            total_reward +=reward\n",
    "            \n",
    "            \n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "                next_state = np.zeros(state.shape)\n",
    "                step = max_steps\n",
    "                \n",
    "                print('Episode: {}'.format(episode),\n",
    "                     'Total reward: {}'.format(total_reward),\n",
    "                     'Training loss: {:.4f}'.format(loss),\n",
    "                     'Explore Prob: {:.4f}'.format(explore_prob))\n",
    "                \n",
    "                rewards_list.append((episode , total_reward))\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((state, action,reward, next_state))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "                \n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                state = next_state\n",
    "                step +=1\n",
    "                \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states_mb = np.array([each[0] for each in batch])\n",
    "            actions_mb = np.array([each[1] for each in batch])\n",
    "            rewards_mb = np.array([each[2] for each in batch])\n",
    "            next_states_mb = np.array([each[3] for each in batch])\n",
    "            \n",
    "            ### TRAINING NETWORK\n",
    "            targetQs = sess.run(mainQN.output, feed_dict={mainQN.inputs_:next_states_mb})\n",
    "            \n",
    "            # Set target_Qs to 0 for states where episode ends \n",
    "            episode_ends = (next_states_mb==np.zeros(states_mb[0].shape)).all(axis=1)\n",
    "            \n",
    "            targetQs[episode_ends]  = (0,0)\n",
    "            \n",
    "            targets = rewards_mb + gamma*np.max(targetQs, axis=1)\n",
    "            \n",
    "            loss , _ = sess.run([mainQN.loss , mainQN.optimizer],\n",
    "                               feed_dict={mainQN.inputs_:states_mb,\n",
    "                                         mainQN.targetQs_: targets,\n",
    "                                         mainQN.actions_: actions_mb})\n",
    "            \n",
    "            \n",
    "    \n",
    "    saver.save(sess, \"checkpoints/cartpole{}.ckpt\".format(train_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Total Reward')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fea947a8748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, rews = np.array(rewards_list).T\n",
    "smoothed_rews = running_mean(rews, 10)\n",
    "plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/cartpole700.ckpt\n"
     ]
    }
   ],
   "source": [
    "test_episodes = 10\n",
    "test_max_steps = 400\n",
    "env.reset()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    for ep in range(1, test_episodes):\n",
    "        t = 0\n",
    "        while t < test_max_steps:\n",
    "            env.render() \n",
    "            \n",
    "            # Get action from Q-network\n",
    "            feed = {mainQN.inputs_: state.reshape((1, *state.shape))}\n",
    "            Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "            action = np.argmax(Qs)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            if done:\n",
    "                t = test_max_steps\n",
    "                env.reset()\n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "#                 print('Episode: {}'.format(ep),\n",
    "#                      'Total reward: {}'.format(total_reward),\n",
    "#                      'Training loss: {:.4f}'.format(loss),\n",
    "#                      'Explore Prob: {:.4f}'.format(explore_prob))\n",
    "                \n",
    "\n",
    "            else:\n",
    "                state = next_state\n",
    "                t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
